# Aufgaben – Lab 5.2 Modelle trainieren & lokal einsetzen

1. **Pro‑und‑Kontra‑Tabelle:** Erstelle eine Tabelle, die Vor‑ und Nachteile von Cloud‑LLMs gegenüber lokalen Modellen gegenüberstellt.  Berücksichtige Aspekte wie Datenschutz, Kosten, Modellqualität, Latenz und Wartungsaufwand.

2. **Ollama installieren:** Installiere Ollama auf deinem Rechner (oder beschreibe die Installation, falls du keinen Zugriff hast).  Starte das Modell `mistral` und führe einen HTTP‑Call durch, der den Unterschied zwischen `Stack` und `Heap` erklärt.  Verwende dazu `curl` oder ein Python‑Script mit `requests`.  Gib die Antwort wieder.

3. **Code‑Modell nutzen:** Starte ein Code‑optimiertes Modell wie `codellama` (oder `starcoder`).  Stelle dem Modell per API die Frage: *„Schreibe eine Python‑Funktion, die die Zahlen 1 bis 100 summiert.“*.  Dokumentiere, welchen Vorteil ein Code‑spezialisiertes Modell gegenüber einem allgemeinen LLM bietet.
